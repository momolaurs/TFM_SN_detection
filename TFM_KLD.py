# -*- coding: utf-8 -*-
"""ADSN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IGtG79BJIETc1KqHh1XNQyW8jAcTVk6Z

# Necessary Libraries (only need to be ran once, it will take approx. 5-10 minutes)
"""
print("Downloading the necessary libraries...")
#spacy
import spacy

#create data frame and count words
import numpy as np
from collections import Counter
import pandas as pd
import math
import re

import matplotlib.pyplot as plt
from tqdm import tqdm

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

import os
import gensim
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from gensim.models import KeyedVectors
import requests
import gzip

import spacy.cli
pipeline_spa = 'es_core_news_lg'
spacy.cli.download(pipeline_spa)

pipeline_cat = "ca_core_news_lg"
spacy.cli.download(pipeline_cat)
print("Done!")

print()

#downloading and loading the model
print("Downloading and the model...")
def download_and_unzip(url, output_path):
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    block_size = 1024  # 1 Kilobyte
    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)
    
    with open(output_path, 'wb') as f:
        for data in response.iter_content(block_size):
            progress_bar.update(len(data))
            f.write(data)
    progress_bar.close()
    
    if total_size != 0 and progress_bar.n != total_size:
        print("ERROR: Something went wrong during the download")
    
    with gzip.open(output_path, 'rb') as f_in:
        with open(output_path[:-3], 'wb') as f_out:
            f_out.write(f_in.read())

if not os.path.exists('cc.es.300.vec.gz'):
    url = "https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz"
    output_path = "cc.es.300.vec.gz"
    download_and_unzip(url, output_path)

print("Loading the model...")

embedding_path = 'cc.es.300.vec'
model = KeyedVectors.load_word2vec_format(embedding_path, limit=500000)

print("Done!")


line = "-" * 40

def open_file(file_name):
    """
    Opens the txt file obtained from CORPES/CREA, skips the first rows with the search query
    data and creates a data frame with the concordances. Keeping only the necessary
    columns for this research (FECHA and CONCORDANCIA).

    Parameters:
        TXT file: Input a .txt file where the columns are separated by tabulation.

    Returns:
        df (DataFrame): A data frame with the concordances of the word and the
            corresponding year.
    """

    df = pd.read_csv(file_name, sep='\t', on_bad_lines = 'warn', quoting=3, skiprows=7)

    hide_columns = ['BIBLIOGRAFÍA', 'AUTOR', 'TÍTULO', 'CRITERIO', 'BLOQUE', 'MEDIO', 'SOPORTE', 'TEMA', 'PAÍS','ZONA', 'TIPOLOGÍA', 'NOTAS']
    df = df.drop(columns=hide_columns)
    df['FECHA'] = pd.to_numeric(df['FECHA'], errors='coerce').astype('Int64')
    #Int64 is used in case one row is empty skip it
    df['CONCORDANCIA'] = df['CONCORDANCIA'].astype(str)

    return df

def lemmatize_data(text):
    """ Lemmatize all words in a string that are not stopwords or punctuation

    Parameters:
        text string: Input a text string

    Returns:
        text string: a new string with the resulting lemmas. """
    nlp = spacy.load('es_core_news_lg')

    text = re.sub(r'[^a-zA-ZáàéèíìóòúùüÁÀÉÈÍÌÓÒÚÙÜñÑçÇ]', " ", text)
    doc = nlp(text.lower())  # Process text with spaCy and convert to lowercase
    lemmatized_text = " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.pos_ == 'NOUN'])
    return lemmatized_text

def lemmatize_data_ca(text):
    """ Lemmatize all words in a string that are not stopwords or punctuation

    Parameters:
        text string: Input a text string

    Returns:
        text string: a new string with the resulting lemmas. """
    nlp = spacy.load("ca_core_news_lg")

    text = re.sub(r'[^a-zA-ZáàéèíìóòúùüÁÀÉÈÍÌÓÒÚÙÜñÑçÇ]', " ", text)
    doc = nlp(text.lower())  # Process text with spaCy and convert to lowercase
    lemmatized_text = " ".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.pos_ == 'NOUN'])
    return lemmatized_text

def count_words(df):
    """
    Count the absolute frequency of all words from the lemmatized data frame

    Parameters:
        df (DataFrame): Input a df that has a column named ['lemmatized_text']
            with the sting you want to count.

    Returns:
        list: A list of tuples containing the words and their absolute frequency.
    """

    # we create a counter
    word_counts = Counter()

    # because it is a df we want to go over all the rows and go over the lemmatized string
    for index, row in df.iterrows():
        lemmatized_text = row['lemmatized_text']

        # we split the sting into individual words and update our counter with the new counts
        word_counts.update(lemmatized_text.split())

    # we transform the counter into a list, this list will contain tuples (word + number count)
    word_occurrences = list(word_counts.items())

    # for better observation we sort the list from highest count to lowest
    word_occurrences.sort(key=lambda x: x[1], reverse=True)

    return word_occurrences

def calculate_relative_frequency(word_occurrences):
    """
    Calculate the relative frequency of each word.

    Parameters:
        list: A list of tuples containing words and their absolute frequency.

    Returns:
        dict: A dictionary where keys are the words and values are their relative frequencies.
    """
    # we get the total amount of words in the list
    total_count = sum(count for word, count in word_occurrences)
    # for each word we divide the frecuency by the total count of words to obtain the relative frequency
    relative_frequency = {word: count / total_count for word, count in word_occurrences}

    return relative_frequency

def kl_divergence(p, q):
    """
    Calculate the Kullback-Leibler (KL) divergence between the two corpus.

    Parameters:
        p (dict): Dictionary with words and relative frequency obtained from the oldest corpus
        q (dict): Dictionary with words and relative frequency obtained from the newest corpus

    Returns:
        float: Final KL divergence between the two corpus.
    """
    #in case the parameters obtained are not dictionaries but lists
    p_dict = dict(p)
    q_dict = dict(q)

    # we want all the unique keys from both dictionaries
    all_keys = set(p_dict.keys()) | set(q_dict.keys())
    # if one of the words doesn't exist in one of dictionries, it is added with a value of 0.0001 to make the division possible
    for word in all_keys:
        p_dict.setdefault(word, 0.0001)
        q_dict.setdefault(word, 0.0001)

    # we calculate the kl divergence of both dictionaries.
    divergence = sum(p_dict[word] * math.log(p_dict[word] / q_dict[word]) for word in all_keys if p_dict[word] != 0 and q_dict[word] != 0)

    return divergence

def plot_year_occurrences(df, year_column, neologism):
    """
    Plots the number of occurrences per year from a specific DataFrame.

    Parameters:
        df (pandas.DataFrame): The DataFrame containing the data.
        year_column (str): The name of the column containing the year data.
        neologism (str): The current target word being analyzed.
    """
    # Count the occurrences of each year
    year_counts = df[year_column].value_counts().sort_index()

    # Plot the data
    plt.figure(figsize=(10, 6))
    plt.plot(year_counts.index, year_counts.values, marker='o')
    plt.title(f'Number of Occurrences Per Year of "{neologism}"')
    plt.xlabel('Year')
    plt.ylabel('Number of Occurrences')
    plt.grid(True)
    plt.show()

def get_top_words(df, word_to_skip, top_n=20):
    """
    Plots the total number of occurrences from a specific DataFrame.

    Parameters:
        df (pandas.DataFrame): The DataFrame containing the data.
        word_to_skip (str): The current target word being analyzed.
        top_n (int): Number of top words you want to show.
    """
    # Tokenize the lemmatized text
    df['word'] = df['lemmatized_text'].apply(word_tokenize)

    # Explode the DataFrame to have one row per token
    exploded_df = df.explode('word')

    # Group by 'word' and count occurrences
    frequency_df = exploded_df.groupby('word').size().reset_index(name='frequency')

    # Sort the frequencies in descending order to show higher frequencies first
    frequency_df = frequency_df.sort_values(by='frequency', ascending=False)

    # Filter out the target word / neologism
    filtered_frequency_df = frequency_df[frequency_df['word'] != word_to_skip]

    # Get the specified number of top words
    top_words = filtered_frequency_df.head(top_n)

    return top_words

#a simple function to return the printing of the words in a more readable form
def get_and_print_top_words(df_1975, df_2023, word_to_skip, get_top_words):

    top_words_1975 = get_top_words(df_1975, word_to_skip)
    top_words_2023 = get_top_words(df_2023, word_to_skip)
    print("Top words for Reference file:")
    print(top_words_1975)
    print("-----------------------")
    print("Top words for Current file:")
    print(top_words_2023)
    print("-----------------------")

    return top_words_1975, top_words_2023

def get_embeddings(word_list, model):
    """
    Get the embeddings of a specific list of words.

    Parameters:
        word_list (list): The list of words of which you want the embeddings
        model : The specific model you want to use to extract the embeddings.
    """
    embeddings = []
    valid_words = []
    #we go through each word in the list, if the word is in the model we save its embedding and also the string with the word
    for word in word_list:
        if word in model:
            embeddings.append(model[word])
            valid_words.append(word)
    return embeddings, valid_words

def scale_embeddings(embeddings, x_range=(-9, 4), y_range=(-9, 9)):
    """
    From a list of embeddings we scale them to fit a plot.

    Parameters:
        embeddings (list): The list of words embeddings.
        x_range : The specific x axis range you want to fit.
        y_range : The specific y axis range you want to fit.
    """
    x_min, x_max = x_range
    y_min, y_max = y_range
    embeddings_min = embeddings.min(axis=0)
    embeddings_max = embeddings.max(axis=0)

    # Scale to [0, 1]
    embeddings_scaled = (embeddings - embeddings_min) / (embeddings_max - embeddings_min)

    # Scale to the desired range
    embeddings_scaled[:, 0] = embeddings_scaled[:, 0] * (x_max - x_min) + x_min
    embeddings_scaled[:, 1] = embeddings_scaled[:, 1] * (y_max - y_min) + y_min

    return embeddings_scaled

def plot_word_embeddings(top_words_1975, top_words_2023, neo, model, get_embeddings, scale_embeddings):
    """
    Create a 2D scatter plot with the top co-occuring words in both concordance files.

    Parameters:
        top_words_1975 (list) : The list with the Reference file top words.
        top_words_2023 (list) : The list with the Current file top words.
        neo (str) : The target word we are analyzing.
        model : the specific model you want to use.
        get_embeddings (func) : A function to collect word embeddings of a list of words.
        scale_embeddings (func) : A fucntion that scales an embedding to fit a plot.
    """
    #we make sure we are using a list
    top_words_1975_list = top_words_1975['word'].tolist()
    top_words_2023_list = top_words_2023['word'].tolist()

    #we get the embeddings of top words that appear in the model and keep the embedding and the list of words
    embeddings_1975, valid_words_1975 = get_embeddings(top_words_1975_list, model)
    embeddings_2023, valid_words_2023 = get_embeddings(top_words_2023_list, model)

    #we merge both the lists of embeddings and word from both files
    all_embeddings = embeddings_1975 + embeddings_2023
    all_words = valid_words_1975 + valid_words_2023
    #we link each list to its label
    labels = ['Reference'] * len(embeddings_1975) + ['Current'] * len(embeddings_2023)

    # we convert the lists to numpy arrays
    all_embeddings_np = np.array(all_embeddings)
    labels_np = np.array(labels)

    # we reduce the dimensionality using PCA transformation
    pca = PCA(n_components=min(all_embeddings_np.shape[1], 40))
    embeddings_pca = pca.fit_transform(all_embeddings_np)
    tsne = TSNE(n_components=2, random_state=0)
    embeddings_2d = tsne.fit_transform(embeddings_pca)

    # we scale the embeddings so they all fit the plot
    embeddings_2d_scaled = scale_embeddings(embeddings_2d)

    # we plot the embeddings
    plt.figure(figsize=(12, 8))
    # we make sure all embeddings appear with colors based on the labels
    for label, color in zip(set(labels_np), ['blue', 'yellow']):
        indices = np.where(labels_np == label)[0]
        plt.scatter(embeddings_2d_scaled[indices, 0], embeddings_2d_scaled[indices, 1], c=color, label=label)

    # we annotate the words
    for i, word in enumerate(all_words):
        plt.annotate(word, xy=(embeddings_2d_scaled[i, 0], embeddings_2d_scaled[i, 1]), fontsize=9)

    # we set fix limits so all plots are the same size
    plt.xlim(-10, 5)
    plt.ylim(-10, 10)

    plt.legend()
    plt.title(f'Embeddings of Top Words around "{neo}" (Reference vs Current)')
    plt.show()

def neo_detection_spa():

    """# Loading file and creating Data Frame (CREA/CORPES)"""
    print("In the space below introduce the name of the term you want to check from the list below (without accentuation or capital letters) so the specific data frames with the concordances can be created.")
    print("AVAILABLE CORPORA: \nagua, alérgico, barbie, bizarro, blanquear, camisa, casa, cuenta, hibernar, ingrediente, lámpara, muro, nube, país, perfil, pingüino, plataforma, seguidor, sofá, televisión, tóxico, tsunami, unicornio, ventana, viral, vaso. \nabridor, abrir, apagón, árbol, arena, barato, barracón, breve, burbuja, caballo, campamento, cielo, clonar, construir, contento, decir, desahogar, descomprimir, enlace, etiqueta, flor, galáctico, gatillar, generoso, grande, húmedo, icono, instalación, java, levantar, liberar, llave, máscara, metal, nuevo, paisa, peligroso, pintar, pitufo, playa, postear, rearmar, remolcar, seguir, sentir, sesión, sol, termómetro, vaca, virtual")
    print()
    neo = input("What word? ")
    print()

    corpus_1975 = "neo_occurrences/tfm test lemma/" + str(neo) + "1975.txt"
    corpus_2023 = "neo_occurrences/tfm test lemma/"+ str(neo) + "2023.txt"
    df_1975 = open_file(corpus_1975)
    df_2023 = open_file(corpus_2023)

    #to check if the data frame has been created correctly
    #print(df_1975)

    #in case I want to look for a test concordance

    #df_1975["CONCORDANCIA"][1]

    #df_2023["CONCORDANCIA"][3]

    """# Lematization, word counting and KL"""

    """ First we preprocess the data by creating a new column in the data frame with
    the context lemmatized and keeping only words that are not stopwords or
    punctuation """

    tqdm.pandas(desc="Lemmatizing Reference data")
    df_1975['lemmatized_text'] = df_1975['CONCORDANCIA'].progress_apply(lemmatize_data)

    tqdm.pandas(desc="Lemmatizing Current data")
    df_2023['lemmatized_text'] = df_2023['CONCORDANCIA'].progress_apply(lemmatize_data)


    """ We continue counting the absolute frequency of each of the words in the context
    of the possible neologism"""

    print("Counting absolute frencuency in both data frames...")
    word_occurrences_1975 = count_words(df_1975)
    word_occurrences_2023 = count_words(df_2023)

    #check if it worked
    #for word, frequency in word_occurrences_1975[:50]:
        #print(f"{word}: {frequency}")

    """ Once we obtained the ablsoute frequency of each word we calculate its relative frequency
    to each corpus """

    print("Calculating relative frequency for both data frames...")
    relative_frequency_1975 = calculate_relative_frequency(word_occurrences_1975)
    relative_frequency_2023 = calculate_relative_frequency(word_occurrences_2023)

    #print(relative_frequency_1975)

    #based on results observed we establish two thresholds for the code to guess the probaility of it being a semantic neologism
    threshold_SN = 0.3
    threshold_prob = 0.25

    p = relative_frequency_1975
    q = relative_frequency_2023

    """ Finally, we calculate the KL divergence between both corpora and print an
    assessment of the probability of the term being a semantic neologism or not. """

    kl_div = kl_divergence(p, q)
    print(line)
    print("KL Divergence:", kl_div)

    if kl_div >= threshold_SN:
        print("\nHigh probabilities of this term being a semantic neologism.")
    elif kl_div >= threshold_prob:
        print("\nIt is possible this term is either a semantic neologism or a word that has suffered a big contextual change.")
    else:
        print("\nThis word has not suffered much contextual change. Indicating that the chances of it being a (recent) semantic neologism are very low.")
    print(line)

    """# Top co-occurrences + plot"""

    frames = [df_1975, df_2023]
    merged_df = pd.concat(frames)
    merged_df['word'] = merged_df['lemmatized_text'].apply(word_tokenize)

    # Explode the DataFrame to have one row per token
    exploded_df = merged_df.explode('word')

    # Group by 'year' and 'tokens', and count occurrences
    frequency_df = exploded_df.groupby(['FECHA', 'word']).size().reset_index(name='frequency')

    # Sort the frequencies in descending order for each year
    frequency_df = frequency_df.sort_values(by=['FECHA', 'frequency'], ascending=[True, False])

    # Define the word to skip
    word_to_skip = neo

    print()
    while True:

        print("1. See top co-occurring words per year")
        print("2. See plot of occurrences per year")
        print("3. See top co-occurring words per concordance file")
        print("4. See 2D scatter plot of top words' embeddings")
        print("X. Go back")
        choice = input("What next? ").upper()
        if choice == "1":
            # Group by 'year' and get top 10 words for each year, skipping the specified word
            top_10_words_by_year = frequency_df.groupby('FECHA').apply(lambda x: x[x['word'] != word_to_skip].head(15))

            # Print each year and its top 10 words
            for year, group in top_10_words_by_year.groupby(level=0):
                print(f"Year: {year}")
                print()
                print(group[['word', 'frequency']].reset_index(drop=True))
                print("-----------------------")
                print()
        elif choice == "2":
            plot_year_occurrences(merged_df, 'FECHA', neo)
        elif choice =="3":
            """We print the top co-occurring words of each concordance file"""
            top_words_1975, top_words_2023 = get_and_print_top_words(df_1975, df_2023, word_to_skip, get_top_words)
        elif choice == "4":
            plot_word_embeddings(top_words_1975, top_words_2023, word_to_skip, model, get_embeddings, scale_embeddings)
        elif choice == "X":
            print("Closing...")
            break
        else:
            print("Invalid choice. Please enter 1, 2, or X.")
            continue


"""# CTILC"""

def neo_detection_cat():
    print("Loading...")
    file_path = "neo_occurrences/concordances_M_Estruch.xlsx"

    # Read the Excel file into a pandas DataFrame
    xls = pd.ExcelFile(file_path)

    # Parse each sheet into a DataFrame
    sheet_names = xls.sheet_names
    data = {}
    for sheet_name in tqdm(sheet_names, desc="Creating data frames..."):
        column_names = ['indx', 'info', 'before', 'neo', 'after']
        # Read the sheet into a DataFrame with specified column names
        data[sheet_name] = pd.read_excel(xls, sheet_name, names=column_names)

    # Now you can access each DataFrame by its sheet name
    # For example, data['Sheet1'] will give you the DataFrame for the 'Sheet1' tab
    while True:
        print()
        print("Add options...")
        neo = input("What word? ").lower()
        print()

        if neo == "x":
            break
        else:
            current_df = "conc_" + str(neo)
            df = data[current_df]
            #print(df)
            df.reset_index(drop=True, inplace=True)

            df['FECHA'] = df['info'].astype(str).str.extract(r'(\d{4})$', expand=False)

            df['CONCORDANCIA'] = df['before'].astype(str) + ' ' + df['neo'] + ' ' + df['after']
            df['CONCORDANCIA'] = df['CONCORDANCIA'].astype(str)
            columns_to_drop = ['indx','info', 'before', 'neo', 'after']
            df = df.drop(columns=columns_to_drop)

            df['FECHA'] = pd.to_numeric(df['FECHA'], errors='coerce')

            # Create two DataFrames based on the year
            df_old = df[df['FECHA'] < 2000]
            df_new = df[df['FECHA'] >= 2000]
            #limit the reference corpus so the period of time analyzed are almost the same size
            df_old = df_old[df_old['FECHA'] >= 1970]

            #print(len(df_old))
            #print(len(df_new))

            tqdm.pandas(desc="Lemmatizing Reference data")
            df_old['lemmatized_text'] = df_old['CONCORDANCIA'].progress_apply(lemmatize_data_ca)

            tqdm.pandas(desc="Lemmatizing Current data")
            df_new['lemmatized_text'] = df_new['CONCORDANCIA'].progress_apply(lemmatize_data_ca)
            print("Lemmatization complete for both datasets.")

            print("Counting absolute frencuency in both data frames...")
            word_occurrences_old = count_words(df_old)
            word_occurrences_new = count_words(df_new)

            print("Calculating relative frequency for both data frames...")
            relative_frequency_old = calculate_relative_frequency(word_occurrences_old)
            relative_frequency_new = calculate_relative_frequency(word_occurrences_new)

            threshold_SN = 0.2
            threshold_prob = 0.15

            p = relative_frequency_old
            q = relative_frequency_new

            print(line)
            kl_div = kl_divergence(p, q)
            print("KL Divergence:", kl_div)

            if kl_div >= threshold_SN:
                print("\nHigh probabilities of this term being a semantic neologism.")
            elif kl_div >= threshold_prob:
                print("\nIt is possible this term is either a semantic neologism or a word that has suffered a big contextual change.")
            else:
                print("\nThis word has not suffered much contextual change. Indicating that the chances of it being a (recent) semantic neologism are very low.")

            print(line)

            frames = [df_old, df_new]
            merged_df = pd.concat(frames)
            merged_df['word'] = merged_df['lemmatized_text'].apply(word_tokenize)

            # Explode the DataFrame to have one row per token
            exploded_df = merged_df.explode('word')

            # Group by 'year' and 'tokens', and count occurrences
            frequency_df = exploded_df.groupby(['FECHA', 'word']).size().reset_index(name='frequency')

            # Sort the frequencies in descending order for each year
            frequency_df = frequency_df.sort_values(by=['FECHA', 'frequency'], ascending=[True, False])

            # Define the word to skip
            word_to_skip = neo

            print()
            while True:

                print("1. See top co-occurring words")
                print("2. See plot of occurrences per year")
                print("3. See top co-occurring words per concordance file")
                print("4. See 2D scatter plot of top words' embeddings")
                print("X. Go back")
                choice = input("What next? ").upper()
                if choice == "1":
                    # Group by 'year' and get top 10 words for each year, skipping the specified word
                    top_10_words_by_year = frequency_df.groupby('FECHA').apply(lambda x: x[x['word'] != word_to_skip].head(15))

                    # Print each year and its top 10 words
                    for year, group in top_10_words_by_year.groupby(level=0):
                        print(f"Year: {year}")
                        print()
                        print(group[['word', 'frequency']].reset_index(drop=True))
                        print("-----------------------")
                        print()
                elif choice == "2":
                    plot_year_occurrences(merged_df, 'FECHA', neo)
                elif choice =="3":
                    """We print the top co-occurring words of each concordance file"""
                    top_words_1975, top_words_2023 = get_and_print_top_words(df_old, df_new, word_to_skip, get_top_words)
                elif choice == "4":
                    plot_word_embeddings(top_words_1975, top_words_2023, word_to_skip, model, get_embeddings, scale_embeddings)
                elif choice == "X":
                    print("Closing...")
                    break
                else:
                    print("Invalid choice. Please enter 1, 2, or X.")
                    continue


if __name__ == "__main__":

    while True:
        print("\n===== WELCOME! =====\n")
        print("1. Semantic Neologism Detector")
        print("X. Close List")
        print("\n")
        choice = input("Enter choice: ").upper()
        if choice == "1":
            language = input("What language? (cat/spa) ").lower()
            if language == "spa":
                neo_detection_spa()
            elif language == "cat":
                neo_detection_cat()
        elif choice == "X":
            print("Closing...")
            break
        else:
            print("Invalid choice. Please enter 1, 2, or X.")
            continue